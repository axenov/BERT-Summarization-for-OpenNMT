alpha: 0.9
batch_size: 1
beam_size: 4
beta: 5.0
block_ngram_repeat: 0
coverage_penalty: summary
data_type: text
dump_beam: ''
image_channel_size: 3
length_penalty: wu
log_file: ''
log_file_level: '0'
max_length: 510
min_length: 55
#ignore_when_blocking: []
model:
- models/en_bert_transformer.pt
n_best: 1
output: output.out
phrase_table: ''
random_sampling_temp: 1.0
random_sampling_topk: 1
ratio: -0.0
sample_rate: 16000
seed: 829
shard_size: 10000
src: src.txt
src_dir: ''
stepwise_penalty: 'true'
verbose: 'true'
window: hamming
window_size: 0.02
window_stride: 0.01
